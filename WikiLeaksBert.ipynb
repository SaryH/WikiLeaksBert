{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCnS_Lg2Xi63",
        "outputId": "1a12ed35-d713-484a-fe48-a698d71478d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import nltk\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "salrEunjgb72"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import random\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize, WordNetLemmatizer\n",
        "\n",
        "\n",
        "def lemmatize_query(query):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(query)\n",
        "    lemmatized_query = ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
        "    return lemmatized_query\n",
        "\n",
        "\n",
        "def Scraper(yourquery):\n",
        "    if \" \" in yourquery:\n",
        "        yourquery = lemmatize_query(yourquery.replace(\" \", \"+\"))\n",
        "\n",
        "    print(\"Lemmatized Query:\"+yourquery)\n",
        "\n",
        "    url = \"https://search.wikileaks.org/?q={}\".format(yourquery)\n",
        "\n",
        "    ua_list = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19577\",\n",
        "        \"Mozilla/5.0 (X11) AppleWebKit/62.41 (KHTML, like Gecko) Edge/17.10859 Safari/452.6\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2656.18 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/44.0.2403.155 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Linux; U; en-US) AppleWebKit/525.13 (KHTML, like Gecko) Chrome/0.2.149.27 Safari/525.13\",\n",
        "        \"Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27\",\n",
        "        \"Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10_5_8; zh-cn) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27\"\n",
        "    ]\n",
        "    ua = random.choice(ua_list)\n",
        "    headers = {'User-Agent': ua}\n",
        "\n",
        "    request = requests.get(url=url, headers=headers)\n",
        "    content = request.text\n",
        "\n",
        "    def findlinks(content):\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "        results = soup.find_all('div', class_='result')\n",
        "        links = []\n",
        "        for result in results:\n",
        "            link = result.find('a', href=True)\n",
        "            if link:\n",
        "                links.append(link['href'])\n",
        "        return links\n",
        "\n",
        "    if request.status_code == 200:\n",
        "        print(\"Request went through.\\n\")\n",
        "        return findlinks(content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4lCXNYaiRNu"
      },
      "source": [
        "# **Gather Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRVcRLcdgV-2",
        "outputId": "7ff3f856-33f3-4d61-8191-859d5cb77e86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Insert the topic of your query:area 51\n",
            "Lemmatized Query:area+51\n",
            "Request went through.\n",
            "\n",
            "['https://wikileaks.org/berats-box/emailid/27744', 'https://wikileaks.org/gifiles/docs/16/1652778_area-51-vets-break-silence-sorry-but-no-space-aliens-.html', 'https://wikileaks.org/gifiles/docs/38/382259_fw-ct-area-51-vets-break-silence-sorry-but-no-space-aliens-.html', 'https://wikileaks.org/gifiles/docs/23/2310498_-military-the-very-secret-history-of-area-51-.html', 'https://wikileaks.org/gifiles/docs/15/1543658_-tactical-the-very-secret-history-of-area-51-.html', 'https://wikileaks.org/gifiles/docs/37/376282_coastzone-area-51-revisited-.html', 'https://wikileaks.org/gifiles/docs/18/1897445_-tactical-npr-org-area-51-uncensored-was-it-ufos-or-the-ussr.html', 'https://wikileaks.org/gifiles/docs/18/1895081_-tactical-windows-1252-q-area_51.html', 'https://wikileaks.org/gifiles/docs/23/2320294_-military-windows-1252-q-area_51.html', 'https://wikileaks.org/plusd/cables/1979MBFRV00636_e.html', 'https://wikileaks.org/hbgary-emails/emailid/6248', 'https://wikileaks.org/wiki/CRS:_Navy_DDG-1000_and_DDG-51_Destroyer_Programs:_Background,_Oversight_Issues,_and_Options_for_Congress,_November_14,_2008', 'https://wikileaks.org/plusd/cables/1978TOKYO04240_d.html', 'https://wikileaks.org/plusd/cables/1978TOKYO06753_d.html', 'https://wikileaks.org/wiki/CRS:_Nonforeign_Cost-of-Living_Allowances_and_Possible_Transition_to_Locality_Pay,_December_1,_2008', 'https://wikileaks.org/plusd/cables/1978STATE162871_d.html', 'https://wikileaks.org/plusd/cables/1978MEXICO14557_d.html', 'https://wikileaks.org/sony/docs/03_02/Finance/SPI/Finance - Animation/2-Monthly Schedules/FY 12/12-12 (March 2012)/Flash/Submission/SPTI - March 2012 International Revenue.xls', 'https://wikileaks.org/spyfiles4/documents/Release-Notes-FinSpy-PC-4.51.pdf', 'https://wikileaks.org/sony/docs/03_03/RISKMGMT/Production Files/2013 Contracts/Blacklist/51-02 21st Realty/Blacklist_51-02 21st Realty.pdf']\n",
            "Error fetching the page for URL: https://wikileaks.org/berats-box/emailid/27744\n",
            "Error details: 401 Client Error: Unauthorized for url: https://wikileaks.org/berats-box/emailid/27744\n",
            "[https://wikileaks.org/gifiles/docs/16/1652778_area-51-vets-break-silence-sorry-but-no-space-aliens-.html] FETCHING...\n",
            "[https://wikileaks.org/gifiles/docs/16/1652778_area-51-vets-break-silence-sorry-but-no-space-aliens-.html] SUCCESS!\n",
            "[https://wikileaks.org/gifiles/docs/38/382259_fw-ct-area-51-vets-break-silence-sorry-but-no-space-aliens-.html] FETCHING...\n",
            "[https://wikileaks.org/gifiles/docs/38/382259_fw-ct-area-51-vets-break-silence-sorry-but-no-space-aliens-.html] SUCCESS!\n",
            "[https://wikileaks.org/gifiles/docs/23/2310498_-military-the-very-secret-history-of-area-51-.html] FETCHING...\n",
            "[https://wikileaks.org/gifiles/docs/23/2310498_-military-the-very-secret-history-of-area-51-.html] SUCCESS!\n",
            "[https://wikileaks.org/gifiles/docs/15/1543658_-tactical-the-very-secret-history-of-area-51-.html] FETCHING...\n",
            "[https://wikileaks.org/gifiles/docs/15/1543658_-tactical-the-very-secret-history-of-area-51-.html] SUCCESS!\n",
            "[https://wikileaks.org/gifiles/docs/37/376282_coastzone-area-51-revisited-.html] FETCHING...\n",
            "[https://wikileaks.org/gifiles/docs/37/376282_coastzone-area-51-revisited-.html] SUCCESS!\n",
            "[https://wikileaks.org/gifiles/docs/18/1897445_-tactical-npr-org-area-51-uncensored-was-it-ufos-or-the-ussr.html] FETCHING...\n",
            "[https://wikileaks.org/gifiles/docs/18/1897445_-tactical-npr-org-area-51-uncensored-was-it-ufos-or-the-ussr.html] SUCCESS!\n",
            "[https://wikileaks.org/gifiles/docs/18/1895081_-tactical-windows-1252-q-area_51.html] FETCHING...\n",
            "[https://wikileaks.org/gifiles/docs/18/1895081_-tactical-windows-1252-q-area_51.html] SUCCESS!\n",
            "[https://wikileaks.org/gifiles/docs/23/2320294_-military-windows-1252-q-area_51.html] FETCHING...\n",
            "[https://wikileaks.org/gifiles/docs/23/2320294_-military-windows-1252-q-area_51.html] SUCCESS!\n",
            "[https://wikileaks.org/plusd/cables/1979MBFRV00636_e.html] FETCHING...\n",
            "Error fetching the page for URL: https://wikileaks.org/hbgary-emails/emailid/6248\n",
            "Error details: 502 Server Error: Bad Gateway for url: https://wikileaks.org/hbgary-emails/emailid/6248\n",
            "[https://wikileaks.org/wiki/CRS:_Navy_DDG-1000_and_DDG-51_Destroyer_Programs:_Background,_Oversight_Issues,_and_Options_for_Congress,_November_14,_2008] FETCHING...\n",
            "[https://wikileaks.org/plusd/cables/1978TOKYO04240_d.html] FETCHING...\n",
            "[https://wikileaks.org/plusd/cables/1978TOKYO06753_d.html] FETCHING...\n",
            "[https://wikileaks.org/wiki/CRS:_Nonforeign_Cost-of-Living_Allowances_and_Possible_Transition_to_Locality_Pay,_December_1,_2008] FETCHING...\n",
            "[https://wikileaks.org/plusd/cables/1978STATE162871_d.html] FETCHING...\n",
            "[https://wikileaks.org/plusd/cables/1978MEXICO14557_d.html] FETCHING...\n",
            "Error fetching the page for URL: https://wikileaks.org/sony/docs/03_02/Finance/SPI/Finance - Animation/2-Monthly Schedules/FY 12/12-12 (March 2012)/Flash/Submission/SPTI - March 2012 International Revenue.xls\n",
            "Error details: 502 Server Error: Bad Gateway for url: https://wikileaks.org/sony/docs/03_02/Finance/SPI/Finance%20-%20Animation/2-Monthly%20Schedules/FY%2012/12-12%20(March%202012)/Flash/Submission/SPTI%20-%20March%202012%20International%20Revenue.xls\n",
            "[https://wikileaks.org/spyfiles4/documents/Release-Notes-FinSpy-PC-4.51.pdf] FETCHING...\n",
            "Error fetching the page for URL: https://wikileaks.org/sony/docs/03_03/RISKMGMT/Production Files/2013 Contracts/Blacklist/51-02 21st Realty/Blacklist_51-02 21st Realty.pdf\n",
            "Error details: 502 Server Error: Bad Gateway for url: https://wikileaks.org/sony/docs/03_03/RISKMGMT/Production%20Files/2013%20Contracts/Blacklist/51-02%2021st%20Realty/Blacklist_51-02%2021st%20Realty.pdf\n",
            "Data saved to 'page_data.csv' successfully.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import random\n",
        "import csv\n",
        "from collections import OrderedDict\n",
        "\n",
        "def sanitize_string(text):\n",
        "    # Define the characters you want to remove or replace\n",
        "    characters_to_remove = ['\\n', '\\r', '\\t', '\"', \",\", '\\\\']  # Newlines, carriage returns, and tabs\n",
        "\n",
        "    # Replace problematic characters with a space\n",
        "    replacement_character = ' '\n",
        "\n",
        "    # Remove or replace the problematic characters\n",
        "    for char in characters_to_remove:\n",
        "        text = text.replace(char, replacement_character)\n",
        "\n",
        "    return text\n",
        "\n",
        "def get_page_content(links):\n",
        "    ua_list = [\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.102 Safari/537.36 Edge/18.19577\",\n",
        "        \"Mozilla/5.0 (X11) AppleWebKit/62.41 (KHTML, like Gecko) Edge/17.10859 Safari/452.6\",\n",
        "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2656.18 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome/44.0.2403.155 Safari/537.36\",\n",
        "        \"Mozilla/5.0 (Linux; U; en-US) AppleWebKit/525.13 (KHTML, like Gecko) Chrome/0.2.149.27 Safari/525.13\",\n",
        "        \"Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27\",\n",
        "        \"Mozilla/5.0 (Macintosh; U; PPC Mac OS X 10_5_8; zh-cn) AppleWebKit/533.20.25 (KHTML, like Gecko) Version/5.0.4 Safari/533.20.27\"\n",
        "    ]\n",
        "    ua = random.choice(ua_list)\n",
        "    headers = {'User-Agent': ua}\n",
        "\n",
        "    all_data = OrderedDict()  # Use OrderedDict to preserve the order of links and data\n",
        "\n",
        "    for url in links:\n",
        "        try:\n",
        "            # Send an HTTP GET request to the URL with the User-Agent header\n",
        "            response = requests.get(url, headers=headers, timeout=5)  # Set a timeout of 5 seconds\n",
        "            response.raise_for_status()  # Check if the request was successful\n",
        "            page_content = response.text\n",
        "\n",
        "            print('['+url+\"] FETCHING...\")\n",
        "\n",
        "            # Parse the HTML content using Beautiful Soup\n",
        "            soup = BeautifulSoup(page_content, 'html.parser')\n",
        "            doc_description = soup.find('div', {'id': 'doc-description'})\n",
        "\n",
        "            if doc_description:\n",
        "                print('[' + url + \"] SUCCESS!\")\n",
        "                page_title = sanitize_string(soup.title.text)\n",
        "                doc_description_text = sanitize_string(doc_description.get_text().strip())\n",
        "                all_data[url] = (page_title, url, doc_description_text)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching the page for URL: {url}\\nError details: {e}\")\n",
        "            continue  # Move to the next link if an exception occurs\n",
        "\n",
        "    return all_data.values()  # Return the values of the OrderedDict\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace this with the list of links you want to fetch\n",
        "    nltk.download('punkt')\n",
        "    query = input(\"Insert the topic of your query:\")\n",
        "    links = Scraper(query)\n",
        "    print(links)\n",
        "\n",
        "    fetched_data = get_page_content(links)\n",
        "\n",
        "    if fetched_data:\n",
        "        # Save the data to a CSV file\n",
        "        with open('page_data.csv', mode='w', encoding='utf-8', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(['Title', 'URL', 'Content'])\n",
        "            writer.writerows(fetched_data)\n",
        "\n",
        "        print(\"Data saved to 'page_data.csv' successfully.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to fetch content for all the links.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g73skAW-iCnW"
      },
      "source": [
        "# **Answer Questions**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Check if GPU is available and set the device accordingly\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)\n",
        "\n",
        "# Replace 'page_data.csv' with the name of your CSV file\n",
        "csv_file = 'page_data.csv'\n",
        "question = input(\"Insert your question:\")\n",
        "# Download stopwords and initialize WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "try:\n",
        "    # Read the CSV file into a Pandas DataFrame\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    # Remove rows with empty 'Link'\n",
        "    df.dropna(subset=['URL'], inplace=True)\n",
        "\n",
        "    # Preprocess the content for BERT question answering\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "    def preprocess_content(row):\n",
        "        context = row['Content']\n",
        "        inputs = tokenizer(question, context, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "        return inputs\n",
        "\n",
        "    # Apply the preprocessing function to each row\n",
        "    df['inputs'] = df.apply(preprocess_content, axis=1)\n",
        "\n",
        "    # Load the pre-trained BERT model for question answering\n",
        "    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    model.to(device)\n",
        "\n",
        "    # Function to perform question answering\n",
        "    def question_answering(inputs):\n",
        "        inputs.to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "\n",
        "        # Get the start and end positions with the highest probability\n",
        "        start_index = torch.argmax(start_logits, dim=1).item()\n",
        "        end_index = torch.argmax(end_logits, dim=1).item()\n",
        "\n",
        "        # Get the answer span from the context\n",
        "        context_tokens = inputs['input_ids'][0]\n",
        "        answer = tokenizer.decode(context_tokens[start_index:end_index+1], skip_special_tokens=True)\n",
        "\n",
        "        return answer\n",
        "\n",
        "    # Perform question answering for each row in the DataFrame\n",
        "    answer_list = []\n",
        "    for _, row in df.iterrows():\n",
        "        answer = question_answering(row['inputs'])\n",
        "        answer_list.append(answer)\n",
        "\n",
        "    # Calculate cosine similarity and append the results to the DataFrame\n",
        "    df['Answer'] = answer_list\n",
        "    df['Cosine Similarity with Question (Content)'] = None\n",
        "    df['Cosine Similarity with Question (Answer)'] = None\n",
        "    df['Cosine Similarity with Question (Title)'] = None\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        content = row['Content']\n",
        "        title = row['Title']\n",
        "        answer = row['Answer']\n",
        "\n",
        "        # Preprocess question, content, and title (remove stop words and lemmatize)\n",
        "        question = ' '.join([lemmatizer.lemmatize(word) for word in question.split() if word.lower() not in stop_words])\n",
        "        content = ' '.join([lemmatizer.lemmatize(word) for word in content.split() if word.lower() not in stop_words])\n",
        "        title = ' '.join([lemmatizer.lemmatize(word) for word in title.split() if word.lower() not in stop_words])\n",
        "\n",
        "        embeddings = tokenizer([question, content, answer, title], return_tensors='pt', padding=True, truncation=True)\n",
        "        embeddings.to(device)\n",
        "        question_embedding = model.base_model(**embeddings)['last_hidden_state'][:, 0].detach().cpu().numpy()\n",
        "        content_embedding = model.base_model(**embeddings)['last_hidden_state'][:, 1].detach().cpu().numpy()\n",
        "        answer_embedding = model.base_model(**embeddings)['last_hidden_state'][:, 2].detach().cpu().numpy()\n",
        "        title_embedding = model.base_model(**embeddings)['last_hidden_state'][:, 3].detach().cpu().numpy()\n",
        "\n",
        "        similarity_question_content = cosine_similarity(question_embedding, content_embedding)[0][0]\n",
        "        similarity_question_answer = cosine_similarity(question_embedding, answer_embedding)[0][0]\n",
        "        similarity_question_title = cosine_similarity(question_embedding, title_embedding)[0][0]\n",
        "\n",
        "        df.at[i, 'Cosine Similarity with Question (Content)'] = similarity_question_content\n",
        "        df.at[i, 'Cosine Similarity with Question (Answer)'] = similarity_question_answer\n",
        "        df.at[i, 'Cosine Similarity with Question (Title)'] = similarity_question_title\n",
        "\n",
        "    # Save the DataFrame to a result CSV file\n",
        "    result_csv_file = 'result_data.csv'\n",
        "    df.to_csv(result_csv_file, index=False)\n",
        "    print(f\"DataFrame saved to '{result_csv_file}' successfully.\")\n",
        "\n",
        "    # Sort the DataFrame based on average cosine similarity and print the top 5 answers\n",
        "    df['Average Cosine Similarity'] = df[\n",
        "        ['Cosine Similarity with Question (Content)', 'Cosine Similarity with Question (Answer)',\n",
        "         'Cosine Similarity with Question (Title)']].mean(axis=1)\n",
        "    sorted_df = df.sort_values(by='Average Cosine Similarity', ascending=False)\n",
        "    top_5_answers = sorted_df.head(5)\n",
        "    pd.set_option('display.max_colwidth', None)  # Set the option to display the entire answer\n",
        "    print(\"\\nTop 5 Answers with Highest Average Cosine Similarity:\")\n",
        "    print(top_5_answers[['URL', 'Answer', 'Average Cosine Similarity']])\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{csv_file}' not found.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(f\"Error: The file '{csv_file}' is empty or contains invalid data.\")\n",
        "except pd.errors.ParserError:\n",
        "    print(f\"Error: Unable to parse the file '{csv_file}'. Check if it's a valid CSV file.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "ab7i7_-ugY0R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00f370c-65de-4d6c-9ad8-47e7946c29b6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Insert your question:are there aliens in area 51?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame saved to 'result_data.csv' successfully.\n",
            "\n",
            "Top 5 Answers with Highest Average Cosine Similarity:\n",
            "                                                                                                             URL  \\\n",
            "0       https://wikileaks.org/gifiles/docs/16/1652778_area-51-vets-break-silence-sorry-but-no-space-aliens-.html   \n",
            "1  https://wikileaks.org/gifiles/docs/38/382259_fw-ct-area-51-vets-break-silence-sorry-but-no-space-aliens-.html   \n",
            "4                                 https://wikileaks.org/gifiles/docs/37/376282_coastzone-area-51-revisited-.html   \n",
            "2               https://wikileaks.org/gifiles/docs/23/2310498_-military-the-very-secret-history-of-area-51-.html   \n",
            "3               https://wikileaks.org/gifiles/docs/15/1543658_-tactical-the-very-secret-history-of-area-51-.html   \n",
            "\n",
            "                                                                                    Answer  \\\n",
            "0                                                                sorry but no space aliens   \n",
            "1                                                                sorry but no space aliens   \n",
            "4                                                                                            \n",
            "2  area 51 will always be associated with conspiracy theories from aliens to time machines   \n",
            "3  area 51 will always be associated with conspiracy theories from aliens to time machines   \n",
            "\n",
            "   Average Cosine Similarity  \n",
            "0                   0.334238  \n",
            "1                   0.334238  \n",
            "4                   0.334238  \n",
            "2                   0.334238  \n",
            "3                   0.334238  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}